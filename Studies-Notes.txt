----- Regression -----


1. Definition: 

-> Is the process of predicting a continuous value
-> Historical data showing details of past to predict 

Ex: CO2 emissions of a car based on other variables and the c02 emission of others car



2. Independent vs Dependent Variable

X -> Independent -> all the atributes/features (columns) except the one you want to predict
Y -> Dependent -> the atribute you want to predict (in the example it would be the C02 Emission)
 


3. Simple vs Mutiple

a) Simple: One independent variable is used to estimate a dependent variable. It can either be linear or non-linear.

b) Multiple: More than one independent variable is used to estimate a dependent variable.



4. Applications of Simple linear regression

-> Sales forecasting; Satisfaction Analysis; Price estimation; Employment income;



5. Regression Algorithms

-> Ordinal; Poisson; Fast Forest quantile; Linear, Polynomial, Lasso, Stepwise, Ridge; Bayesian; Neural Network; Decision Forest; KNN; Boosted decision tree;


6. How to find the best fit?

---- Simple linear regression => y = theta0 + theta1*x1 ------

-> For a simple linear regression you gotta find the minimum MSE (mean square error), to get the line where the residual error is the least as possible.
-> You can use a mathematic formula to find theta0 and theta1 (coeficients of the fit line): 
   - theta1: calculate the mean of x and y, then calculate ((x-xmean)*(y-ymean))/((x-xmean)^2)
   - theta0: ymean - theta1*xmean


7. Multiple linear regression

a) Applications
-> Independent variables effectiveness on prediction; Predictinf impact of changes

b) The best fit

C02_Emissions = theta0 + theta1*Engine_Size + theta2*Cylinders + ...

-> the best fit is the one that leads to a model with the least erros (minimize MSE)
-> The MSE is the mean of all the residual erros, which are the result of the subtraction of the predicted value and the actual value
-> Ordinary Least Squares -> Linear Algebra operations -> too slow, must be used only for data with less than 10k rows
-> An optamization algorithm such as gradient descent; -> proper aproach for larger datasets.


8. Model Evaluation Approaches

a) Train and Test on the same Dataset

-> Use the total dataset to traind and than take a piece of the dataset to test the model 
-> One of the simplest way to calculate the accuracy of the model is to take the mean of all the errors (value predicted - actual value)
-> May result in over-fitting -> the model is overly trained to the dataset, which may capture noise and produce a non-generalized model

b) Train/Test Split

-> Select a portion of the dataset to train and the rest to test.
-> Involving splitting the data between train and test set.
-> More accurate evaluation on out-of-sample accuracy.
-> Better for real world.
-> Highly dependent on which datasets the data is trained and tested

c) K-Fold Cross-Validation

-> If you are using a 4 fold cross validation for example you can use the first 25% to testing and the rest to train
then you use the second quarter for testing and the rest for training and that goes till the end of all folds.
After doing that you average the accuracy of all this tests and trainings


----------------------------------- // -------------------------------------





 